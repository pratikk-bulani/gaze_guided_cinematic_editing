{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "({1: [0, 4, 6], 2: [1, 3, 5], 3: [2]},\n ['../theatre_dataset/braunfels_1/shots/braunfels_1-p1-MS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p2-FS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p2-p3-FS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p3-FS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p2-MS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p2-p3-FS.txt',\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p3-MS.txt'],\n {'../theatre_dataset/braunfels_1/shots/braunfels_1-p1-MS.txt': 0,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p2-FS.txt': 1,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p2-p3-FS.txt': 2,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-p3-FS.txt': 3,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p2-MS.txt': 4,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p2-p3-FS.txt': 5,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p3-MS.txt': 6,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p1-FS.txt': 0,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p2-FS.txt': 4,\n  '../theatre_dataset/braunfels_1/shots/braunfels_1-p3-FS.txt': 6},\n ['p1', 'p2', 'p3'],\n {'p1': 0, 'p2': 1, 'p3': 2})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_FOLDER = '../theatre_dataset/'\n",
    "VIDEO_FOLDER = 'braunfels_1'\n",
    "OFFSET_EYE_GAZE = 2 # seconds\n",
    "DICT_NOACTORS_SHOTFILE = dict() # key = no of actors in a shot, value = list of indices where these shots are present in the LIST_FILENAMEPATH_SHOTS\n",
    "LIST_FILENAMEPATH_SHOTS = sorted(glob.glob(DATASET_FOLDER + VIDEO_FOLDER + \"/shots/*.txt\"))\n",
    "for shot_filename in LIST_FILENAMEPATH_SHOTS[::]:\n",
    "    no_of_actors = shot_filename.count(\"-\") - 1\n",
    "    if no_of_actors == 1 and shot_filename.count(\"FS\") == 1:\n",
    "        LIST_FILENAMEPATH_SHOTS.remove(shot_filename)\n",
    "    elif no_of_actors > 1 and shot_filename.count(\"MS\") == 1:\n",
    "        LIST_FILENAMEPATH_SHOTS.remove(shot_filename)\n",
    "DICT_FILENAMEPATH_SHOTNO = {LIST_FILENAMEPATH_SHOTS[i]:i for i in range(len(LIST_FILENAMEPATH_SHOTS))}\n",
    "for i in range(len(LIST_FILENAMEPATH_SHOTS)):\n",
    "    shot_filename = LIST_FILENAMEPATH_SHOTS[i]\n",
    "    no_of_actors = shot_filename.count(\"-\") - 1\n",
    "    if no_of_actors in DICT_NOACTORS_SHOTFILE:\n",
    "        DICT_NOACTORS_SHOTFILE[no_of_actors].append(i)\n",
    "    else:\n",
    "        DICT_NOACTORS_SHOTFILE[no_of_actors] = [i]\n",
    "LIST_ACTORNAMES = [i[i.rindex('/')+len(VIDEO_FOLDER)+2:-4] for i in sorted(glob.glob(DATASET_FOLDER + VIDEO_FOLDER + \"/tracks/*.txt\"))]\n",
    "DICT_ACTORNAME_ACTORNO = {LIST_ACTORNAMES[i]:i for i in range(len(LIST_ACTORNAMES))}\n",
    "for i in DICT_NOACTORS_SHOTFILE[1]:\n",
    "    shot_filename = LIST_FILENAMEPATH_SHOTS[i]\n",
    "    shot_filename_2 = shot_filename.replace(\"MS\", \"FS\")\n",
    "    DICT_FILENAMEPATH_SHOTNO[shot_filename_2] = DICT_FILENAMEPATH_SHOTNO[shot_filename]\n",
    "DICT_NOACTORS_SHOTFILE, LIST_FILENAMEPATH_SHOTS, DICT_FILENAMEPATH_SHOTNO, LIST_ACTORNAMES, DICT_ACTORNAME_ACTORNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_video_input = cv2.VideoCapture(DATASET_FOLDER + VIDEO_FOLDER + \"/\" + VIDEO_FOLDER + \".mp4\")\n",
    "VIDEO_INPUT_CODEC = int(file_video_input.get(cv2.CAP_PROP_FOURCC)) # not working\n",
    "VIDEO_INPUT_FRAME_SIZE = (int(file_video_input.get(cv2.CAP_PROP_FRAME_WIDTH)), int(file_video_input.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "VIDEO_INPUT_FPS = file_video_input.get(cv2.CAP_PROP_FPS)\n",
    "VIDEO_INPUT_FRAMES_COUNT = min(int(file_video_input.get(cv2.CAP_PROP_FRAME_COUNT)), *[np.loadtxt(i).shape[0] for i in LIST_FILENAMEPATH_SHOTS])\n",
    "OFFSET_EYE_GAZE = int(np.rint(OFFSET_EYE_GAZE * VIDEO_INPUT_FPS)) # index number\n",
    "file_video_input.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(828601953, (3840, 2160), 29.97002997002997, 2276, 60)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_INPUT_CODEC, VIDEO_INPUT_FRAME_SIZE, VIDEO_INPUT_FPS, VIDEO_INPUT_FRAMES_COUNT, OFFSET_EYE_GAZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276, 4, 7)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is coordinates of all the shots\n",
    "all_shots_coords = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, 4, len(LIST_FILENAMEPATH_SHOTS))) # dimensions = (#frames, coordinates (x1, y1, x2, y2), #shots)\n",
    "for i in range(len(LIST_FILENAMEPATH_SHOTS)):\n",
    "    all_shots_coords[:, :, i] = np.loadtxt(LIST_FILENAMEPATH_SHOTS[i])[:VIDEO_INPUT_FRAMES_COUNT]\n",
    "all_shots_coords = np.rint(all_shots_coords).astype(np.int16)\n",
    "all_shots_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276, 3, 2)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t_i = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, len(DICT_NOACTORS_SHOTFILE[1]), 2)) # center of all the shots across all the frames # dimensions = (#frames, #shots (1-actor), coordinates (x,y))\n",
    "for i in range(len(DICT_NOACTORS_SHOTFILE[1])):\n",
    "    temp = np.loadtxt(LIST_FILENAMEPATH_SHOTS[DICT_NOACTORS_SHOTFILE[1][i]])[:VIDEO_INPUT_FRAMES_COUNT]\n",
    "    c_t_i[:, i, 0] = (temp[:, 0] + temp[:, 2]) / 2\n",
    "    c_t_i[:, i, 1] = (temp[:, 1] + temp[:, 3]) / 2\n",
    "c_t_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276, 5, 2)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATASET_FOLDER + VIDEO_FOLDER + \"/\" + VIDEO_FOLDER + \"_gaze.yml\", 'r') as f:\n",
    "    dict_user_gaze = yaml.safe_load(f) # key = user number, value = list of lists\n",
    "g_t_k = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, len(dict_user_gaze), 2)) # gaze coordinates of all the users # dimensions = (#frames, #users, coordinates (x, y))\n",
    "for i, x_y_coords in dict_user_gaze.items():\n",
    "    temp = np.array(x_y_coords).astype(float)[OFFSET_EYE_GAZE:OFFSET_EYE_GAZE+VIDEO_INPUT_FRAMES_COUNT, :2]\n",
    "    g_t_k[:, i, :] = temp\n",
    "g_t_k[:, :, 0] = np.rint(g_t_k[:, :, 0] / 1920 * VIDEO_INPUT_FRAME_SIZE[0])\n",
    "g_t_k[:, :, 1] = np.rint(g_t_k[:, :, 1] / 1080 * VIDEO_INPUT_FRAME_SIZE[1])\n",
    "g_t_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276, 3)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_t_i = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, len(DICT_NOACTORS_SHOTFILE[1]))) # distance of shot centers and gaze # dimensions = (#frames, #shots (1-actor))\n",
    "for i in range(len(DICT_NOACTORS_SHOTFILE[1])):\n",
    "    d_t_i[:, i] = np.sum(np.linalg.norm(c_t_i[:, [i], :] - g_t_k, axis = 2), axis = 1)\n",
    "d_t_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the bounding box\n",
    "bx1_t_orig = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, 0), dtype = np.int16) # dimensions = (#frames, #actors)\n",
    "bx2_t_orig = np.empty(shape = (VIDEO_INPUT_FRAMES_COUNT, 0), dtype = np.int16) # dimensions = (#frames, #actors)\n",
    "for file_name in sorted(glob.glob(DATASET_FOLDER + VIDEO_FOLDER + \"/tracks/*.txt\")):\n",
    "     try:\n",
    "          temp = np.rint(np.loadtxt(file_name, delimiter = ',')[:VIDEO_INPUT_FRAMES_COUNT]).astype(np.int16)\n",
    "     except:\n",
    "          temp = np.rint(np.loadtxt(file_name)[:VIDEO_INPUT_FRAMES_COUNT]).astype(np.int16)\n",
    "     bx1_t_orig = np.append(bx1_t_orig, temp[:, [0]], axis = 1)\n",
    "     bx2_t_orig = np.append(bx2_t_orig, temp[:, [2]], axis = 1)\n",
    "x_t = (bx1_t_orig + bx2_t_orig) // 2 # dimensions = (#frames, #actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276, 7)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gs_t_i = -1 * np.ones(shape = (VIDEO_INPUT_FRAMES_COUNT, len(LIST_FILENAMEPATH_SHOTS))) # gaze potential # dimensions = (#frames, #shots)\n",
    "temp_1 = 1 / d_t_i\n",
    "temp_2 = temp_1.sum(axis = 1)\n",
    "for i in range(len(DICT_NOACTORS_SHOTFILE[1])):\n",
    "    Gs_t_i[:, DICT_NOACTORS_SHOTFILE[1][i]] = temp_1[:, i] / temp_2\n",
    "Gs_t_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    if i not in DICT_NOACTORS_SHOTFILE: break\n",
    "    for j in range(len(DICT_NOACTORS_SHOTFILE[i])):\n",
    "        list_shot_filename = LIST_FILENAMEPATH_SHOTS[DICT_NOACTORS_SHOTFILE[i][j]].split(\"-\")\n",
    "        list_actors_shot = np.array([DICT_ACTORNAME_ACTORNO[k] for k in list_shot_filename[1:-1]])\n",
    "        actors_left_right = x_t[:, list_actors_shot].argsort(axis = 1)\n",
    "        actors_left_right = list_actors_shot[actors_left_right]\n",
    "        temp_1 = np.sort(actors_left_right[:, :-1], axis = 1)\n",
    "        temp_2 = np.sort(actors_left_right[:, 1:], axis = 1)\n",
    "        temp_1 = np.vectorize(lambda t: LIST_ACTORNAMES[t])(temp_1)\n",
    "        temp_2 = np.vectorize(lambda t: LIST_ACTORNAMES[t])(temp_2)\n",
    "        temp_1 = np.apply_along_axis(lambda t: list_shot_filename[0] + '-' + '-'.join(t) + '-' + list_shot_filename[-1], 1, temp_1)\n",
    "        temp_2 = np.apply_along_axis(lambda t: list_shot_filename[0] + '-' + '-'.join(t) + '-' + list_shot_filename[-1], 1, temp_2)\n",
    "        temp_1 = np.vectorize(lambda t: DICT_FILENAMEPATH_SHOTNO[t])(temp_1)\n",
    "        temp_2 = np.vectorize(lambda t: DICT_FILENAMEPATH_SHOTNO[t])(temp_2)\n",
    "        Gs_t_a = Gs_t_i[range(VIDEO_INPUT_FRAMES_COUNT), temp_1]\n",
    "        Gs_t_b = Gs_t_i[range(VIDEO_INPUT_FRAMES_COUNT), temp_2]\n",
    "        assert np.all(Gs_t_a != -1) and np.all(Gs_t_b != -1)\n",
    "        Gs_t_i[:, DICT_NOACTORS_SHOTFILE[i][j]] = Gs_t_a + Gs_t_b - np.abs(Gs_t_a - Gs_t_b)\n",
    "assert np.all(Gs_t_i != -1)\n",
    "Gs_t_i = -np.log(Gs_t_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.zeros(shape = (len(LIST_FILENAMEPATH_SHOTS), len(LIST_FILENAMEPATH_SHOTS), VIDEO_INPUT_FRAMES_COUNT-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shot transition cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 2\n",
    "shot_transition_cost = np.empty(shape = edges.shape)\n",
    "for i in range(VIDEO_INPUT_FRAMES_COUNT-1):\n",
    "    shot_transition_cost[:, :, i] = np.diagflat([-lamb]*len(LIST_FILENAMEPATH_SHOTS), 0)\n",
    "shot_transition_cost += lamb\n",
    "edges += shot_transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the IOU of the two rectangles\n",
    "def bb_intersection_over_union(boxA, boxB): # here parameter box = (x1, y1, x2, y2)\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2; beta = 0.4; mu = 1; v = 1000\n",
    "overlap_cost = np.empty(shape = edges.shape)\n",
    "for i in range(VIDEO_INPUT_FRAMES_COUNT-1): # iterate along the time frames\n",
    "    for j in range(len(LIST_FILENAMEPATH_SHOTS)): # iterate along the current shots\n",
    "        for k in range(len(LIST_FILENAMEPATH_SHOTS)): # iterate along the next shots\n",
    "            gamma = bb_intersection_over_union(all_shots_coords[i, :, j], all_shots_coords[i+1, :, k])\n",
    "            if j == k:\n",
    "                overlap_cost[j, k, i] = 0\n",
    "            elif gamma >= 0 and gamma < alpha:\n",
    "                overlap_cost[j, k, i] = 0\n",
    "            elif gamma >= alpha and gamma <= beta:\n",
    "                overlap_cost[j, k, i] = mu * gamma / alpha\n",
    "            elif gamma > beta and gamma <= 1:\n",
    "                overlap_cost[j, k, i] = v\n",
    "            else:\n",
    "                raise Exception(\"Error: Invalid gamma value. Valid range of gamma: [0, 1]\")\n",
    "edges += overlap_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhythm cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will do later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cost = np.zeros(shape = (VIDEO_INPUT_FRAMES_COUNT, len(LIST_FILENAMEPATH_SHOTS)))\n",
    "graph_cost[0, :] = Gs_t_i[0, :]\n",
    "back_trace = -1 * np.ones(shape = (VIDEO_INPUT_FRAMES_COUNT, len(LIST_FILENAMEPATH_SHOTS)))\n",
    "for i in range(1, VIDEO_INPUT_FRAMES_COUNT):\n",
    "    for j in range(len(LIST_FILENAMEPATH_SHOTS)):\n",
    "        temp = graph_cost[i-1, :] + edges[:, j, i-1] + Gs_t_i[i, j]\n",
    "        graph_cost[i, j] = temp.min()\n",
    "        back_trace[i, j] = temp.argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2276,)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_t = -1 * np.ones(shape = (VIDEO_INPUT_FRAMES_COUNT)).astype(np.int16)\n",
    "result_t[-1] = graph_cost[-1, :].argmin()\n",
    "for i in range(VIDEO_INPUT_FRAMES_COUNT-1, 0, -1):\n",
    "    result_t[i-1] = back_trace[i, result_t[i]]\n",
    "result_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_VIDEO_FRAME_SIZE = (1280, 720)\n",
    "file_output_video = cv2.VideoWriter(DATASET_FOLDER + VIDEO_FOLDER + \"/\" + 'gazed.mp4', cv2.VideoWriter_fourcc(*'mp4v'), VIDEO_INPUT_FPS, OUTPUT_VIDEO_FRAME_SIZE)\n",
    "file_input_video = cv2.VideoCapture(DATASET_FOLDER + VIDEO_FOLDER + \"/\" + VIDEO_FOLDER + \".mp4\")\n",
    "for i in range(VIDEO_INPUT_FRAMES_COUNT):\n",
    "    success, img_frame = file_input_video.read()\n",
    "    temp_x1, temp_y1, temp_x2, temp_y2 = all_shots_coords[i, :, result_t[i]]\n",
    "    file_output_video.write(cv2.resize(img_frame[temp_y1:temp_y2+1, temp_x1:temp_x2+1], OUTPUT_VIDEO_FRAME_SIZE))\n",
    "file_input_video.release()\n",
    "file_output_video.release()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8444844a564d5f7966954b32229d18f1d875f0f75a6a38ef4ed637ba8d6d479"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}